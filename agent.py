import json
import os
from google import genai
from google.genai import types

class CodingAgent:
    """
    Agente de Codificaci√≥n capaz de interactuar con el sistema de archivos.
    Utiliza el modelo Gemini para razonamiento y Function Calling.
    """
    def __init__(self, client: genai.Client):
        self.client = client
        
        # 1. Instrucci√≥n del Sistema (System Instruction)
        self.system_instruction = (
            "Eres un experto asistente de codificaci√≥n que opera en un sistema Linux Mint. "
            "Tu tarea es ayudar al usuario a gestionar, leer y editar archivos. "
            "DEBES usar tus herramientas (list_files_in_dir, read_file, edit_file) para interactuar con el ambiente. "
            "Razona sobre los pasos necesarios antes de responder. S√© conciso."
        )

        # 2. Inicializaci√≥n de la Memoria (Historial de Mensajes)
        self.messages = []
        
        # 3. Inicializaci√≥n de Herramientas
        # El SDK de Gemini las infiere autom√°ticamente de los docstrings
        self.tools = self.setup_tools() 

    # --- Herramientas (Functions/Tools) del Agente ---

    def list_files_in_dir(self, directory: str = ".") -> str:
        """
        Lista los archivos y directorios dentro de la ruta especificada. 
        √ötil para explorar el ambiente de trabajo del agente.
        :param directory: La ruta del directorio a listar. Por defecto es el directorio actual (.).
        :return: Una cadena JSON que contiene la lista de archivos o un mensaje de error.
        """
        print(f"üõ†Ô∏è Ejecutando: list_files_in_dir en {directory}")
        try:
            files = os.listdir(directory)
            return json.dumps({"status": "success", "files": files})
        except FileNotFoundError:
            return json.dumps({"status": "error", "message": f"Directorio no encontrado: {directory}"})
        except Exception as e:
            return json.dumps({"status": "error", "message": f"Error al listar archivos: {e}"})

    def read_file(self, path: str) -> str:
        """
        Lee el contenido completo de un archivo en la ruta especificada. 
        √ötil para entender el c√≥digo o texto actual de un archivo antes de modificarlo.
        :param path: La ruta completa del archivo a leer.
        :return: Una cadena JSON que contiene el contenido del archivo o un mensaje de error.
        """
        print(f"üõ†Ô∏è Ejecutando: read_file en {path}")
        try:
            with open(path, 'r') as f:
                content = f.read()
            return json.dumps({"status": "success", "content": content})
        except FileNotFoundError:
            return json.dumps({"status": "error", "message": f"Archivo no encontrado: {path}"})
        except Exception as e:
            return json.dumps({"status": "error", "message": f"Error al leer archivo: {e}"})
            
    def edit_file(self, path: str, new_text: str, old_text: str = None) -> str:
        """
        Edita o crea un archivo. Si 'old_text' se proporciona, reemplaza esa ocurrencia con 'new_text' 
        (√∫til para reemplazar funciones espec√≠ficas). Si 'old_text' es None, reemplaza el contenido 
        completo (o crea el archivo si no existe).
        :param path: La ruta del archivo a editar/crear.
        :param new_text: El texto que se usar√° para reemplazar el 'old_text' o el contenido completo.
        :param old_text: (Opcional) El texto a ser reemplazado.
        :return: Una cadena JSON que indica el resultado de la operaci√≥n.
        """
        print(f"üõ†Ô∏è Ejecutando: edit_file en {path}. ¬øReemplazo? {bool(old_text)}")
        try:
            # 1. Manejo de reemplazo vs. creaci√≥n/sobreescritura
            if os.path.exists(path) and old_text:
                with open(path, 'r') as f:
                    content = f.read()
                
                if old_text not in content:
                    return json.dumps({"status": "error", "message": "El texto a reemplazar (old_text) no fue encontrado."})
                
                content = content.replace(old_text, new_text, 1) # Reemplaza solo la primera ocurrencia
                action = "editado (reemplazo)"
            elif new_text:
                # 2. Creaci√≥n o sobreescritura total
                content = new_text
                action = "creado/sobreescrito"
            else:
                return json.dumps({"status": "error", "message": "Se requiere new_text para crear o editar."})

            # 3. Escritura del archivo
            # Asegurar que el directorio exista (√∫til si el agente intenta crear en subcarpetas)
            os.makedirs(os.path.dirname(path) or '.', exist_ok=True) 
            with open(path, 'w') as f:
                f.write(content)
                
            return json.dumps({"status": "success", "message": f"Archivo {path} {action} correctamente."})
            
        except Exception as e:
            return json.dumps({"status": "error", "message": f"Error al editar/crear archivo: {e}"})

    def setup_tools(self):
        """
        Devuelve la lista de funciones de Python que el modelo puede llamar.
        """
        return [
            self.list_files_in_dir, 
            self.read_file, 
            self.edit_file
        ]
        
    def process_response(self, user_input: str):
        """
        Bucle de razonamiento iterativo: gestiona la comunicaci√≥n con Gemini, la 
        ejecuci√≥n de herramientas y la memoria del historial.
        """
        # 1. A√±adir la entrada del usuario al historial
        self.messages.append(user_input)

        # Bucle interno: permite al agente ejecutar m√∫ltiples herramientas en una sola interacci√≥n
        while True:
            # 2. Llamada al Modelo con Historial, Instrucci√≥n del Sistema y Herramientas
            response = self.client.models.generate_content(
                model='gemini-2.5-flash', # Un modelo r√°pido y capaz de Function Calling
                contents=self.messages,
                config=types.GenerateContentConfig(
                    system_instruction=self.system_instruction,
                    tools=self.tools
                )
            )

            # 3. Procesamiento de la Respuesta del Modelo
            if not response.function_calls:
                # Si no hay llamadas a funci√≥n, la respuesta es texto final
                
                # 3a. A√±adir la respuesta final del modelo al historial (Memoria)
                self.messages.append(response.text)
                
                print(f"\nü§ñ Asistente:\n{response.text}\n")
                return # Salir del bucle, la conversaci√≥n contin√∫a en main.py

            # 4. Manejo de Llamadas a Funci√≥n (Function Calling)
            tool_results = []
            
            for function_call in response.function_calls:
                function_name = function_call.name
                args = dict(function_call.args) # Argumentos dict
                
                print(f"--- Llamada del Modelo ---")
                print(f"üõ†Ô∏è Funci√≥n solicitada: {function_name}")
                print(f"üì¶ Argumentos: {args}")

                # B√∫squeda y Ejecuci√≥n de la Herramienta de Python
                if hasattr(self, function_name):
                    tool_function = getattr(self, function_name)
                    
                    # 5. Ejecuci√≥n real de la funci√≥n
                    result = tool_function(**args)
                    
                    # 6. Preparar el Resultado para Devolverlo al Modelo
                    tool_results.append(
                        types.Content(
                            role='tool',
                            parts=[types.Part.from_function_response(name=function_name, response={'result': result})]
                        )
                    )
                else:
                    error_msg = f"Error: Funci√≥n {function_name} no implementada."
                    tool_results.append(
                        types.Content(
                            role='tool',
                            parts=[types.Part.from_function_response(name=function_name, response={'error': error_msg})]
                        )
                    )
            
            # 7. Re-Llamada al Modelo
            # Se adjunta la respuesta original (que incluye la solicitud de funci√≥n) y el 
            # resultado de la herramienta (role='tool') para el razonamiento en la siguiente llamada.
            self.messages.append(response)  # La solicitud del modelo (qu√© quiere hacer)
            self.messages.extend(tool_results) # El resultado de la ejecuci√≥n (qu√© pas√≥ realmente)
            
            # El bucle 'while True' contin√∫a, llamando de nuevo a Gemini con el historial extendido.